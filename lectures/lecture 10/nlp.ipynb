{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd",
   "metadata": {
    "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd"
   },
   "source": [
    "# X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v√† khai ph√° d·ªØ li·ªáu vƒÉn b·∫£n\n",
    "\n",
    "![](https://www.thuatngumarketing.com/wp-content/uploads/2017/12/NLP.png.pagespeed.ce_.1YNuw_5dJH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c",
   "metadata": {
    "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c"
   },
   "source": [
    "## ƒêiÃ£nh nghiÃÉa\n",
    "\n",
    "> **X·ª≠ l√≠ ng√¥n ng·ªØ t·ª± nhi√™n**: S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t ph√¢n t√≠ch v√† l√†m s·∫°ch d·ªØ li·ªáu k·∫øt h·ª£p v·ªõi m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ khai th√°c th√¥ng tin trong d·ªØ li·ªáu ng√¥n ng·ªØ.\n",
    "\n",
    "> **D·ªØ li·ªáu ng√¥n ng·ªØ**: N√≥i cho nhau nghe (d·ªØ li·ªáu ng√¥n ng·ªØ √¢m thanh) v√† vi·∫øt cho nhau ƒë·ªçc (d·ªØ li·ªáu ng√¥n ng·ªØ vƒÉn b·∫£n)\n",
    "\n",
    "> **Text Mining**: K·ªπ thu·∫≠t x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng vƒÉn b·∫£n v√† khai th√°c th√¥ng tin t·ª´ d·ªØ li·ªáu vƒÉn b·∫£n.\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0108.png)\n",
    "\n",
    "## ∆ØÃÅng duÃ£ng\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0101.png)\n",
    "\n",
    "> Tham kh·∫£o: [Practical Natural Language Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana\n",
    "](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092",
   "metadata": {
    "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092"
   },
   "source": [
    "## Quy triÃÄnh\n",
    "\n",
    "> 1. **Preprocessing** (Ti·ªÅn x·ª≠ l√Ω) (L√†m s·∫°ch & Chu·∫©n ho√° d·ªØ li·ªáu)\n",
    "> 2. **Tokenizing** (T√°ch t·ª´)\n",
    "> 3. **Vectorizing** (Vect∆° h√≥a)\n",
    "> 4. **Modeling** (X√¢y d·ª±ng m√¥ h√¨nh)\n",
    "> 5. **Intepreting result & Application** (·ª®ng d·ª•ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b914-9acd-4e9f-9072-a6663fea3313",
   "metadata": {
    "id": "4de5b914-9acd-4e9f-9072-a6663fea3313"
   },
   "source": [
    "## C√¥ng c·ª• v√† th∆∞ vi·ªán\n",
    "\n",
    "1. Python Nature Language Toolkit ([Python NLTK](https://www.nltk.org/)) h·ªó tr·ª£ x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng vƒÉn b·∫£n.\n",
    "2. Gensim/Tensorflow/Scikit-learn h·ªó tr·ª£ x√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358d2f3-fa62-4785-9079-7483d7e139fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797",
   "metadata": {
    "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797"
   },
   "source": [
    "#### 1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6231fa4-3399-444d-821e-c232c546d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca6e150-9835-4271-b501-28a591c44702",
   "metadata": {
    "id": "651749cb-7a5f-4843-b816-67b5a34effde"
   },
   "source": [
    "#### TODO: L√†m s·∫°ch d·ªØ li·ªáu ch·ªâ gi·ªØ l·∫°i d·ªØ li·ªáu b·∫±ng ch·ªØ\n",
    "1. N·∫øu l√† d·ªØ li·ªáu scrapped t·ª´ web th√¨ ch·ªâ tr√≠ch l·ªçc d·ªØ li·ªáu ch·ª© (text only)\n",
    "2. Lo·∫°i b·ªè hyperlink (n·∫øu c√≥) # https://baophapluat.com # l·∫´n th√†nh t·ª´ word \n",
    "3. N·∫øu l√† d·ªØ li·ªáu web th√¨ c·∫ßn lo·∫°i b·ªè emoji # :smile: :angry:\n",
    "4. LoaÃ£i b·ªè t·∫•t c·∫£ c√°c d·∫•u (.,\\/!@#$%^&*()+_ etc.)\n",
    "```\n",
    "r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\‚Äú\\‚Äù\\\\\\-\\:‚Ä¶&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\‚Ä¢]'\n",
    "```\n",
    "5. Lo·∫°i b·ªè t·∫•t c·∫£ c√°c s·ªë\n",
    "6. Lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng v√† ƒë·ªïi text th√†nh lowercase # normalize # T == t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3c6212-7cfb-430c-8a0e-94866e79eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''<head></head>\n",
    "<body>\n",
    "    <p>\n",
    "    Here's 1 paragraph of text üî•. !\n",
    "    <a href=\"https://www.dataquest.io\">Learn Data Science Online ü§°</a>\n",
    "    </p>\n",
    "    <p>\n",
    "    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\n",
    "    <a href=\"https://www.python.org\">Python 101</a> </p>\n",
    "</body></html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b68c79-1881-42a6-810c-578ced6c6dd1",
   "metadata": {},
   "source": [
    "#### 2. T√°ch t·ª´ (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3028c-bcd7-4a31-8c36-2ba240740457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a08781-af2e-4d77-b24c-6a18078d7743",
   "metadata": {},
   "source": [
    "#### T·ªïng h·ª£p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54974ecd-bd75-4687-8a56-75355af6d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): # l√†m s·∫°ch d·ªØ li·ªáu t·ª´ ƒë·∫ßu t·ªõi cu·ªëi\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b093abad-5a08-479b-be6c-114a94e76469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>(T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...</td>\n",
       "      <td>S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...</td>\n",
       "      <td>T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...</td>\n",
       "      <td>Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...</td>\n",
       "      <td>Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...</td>\n",
       "      <td>S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...</td>\n",
       "      <td>ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...</td>\n",
       "      <td>L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "1  Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...   \n",
       "2  ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...   \n",
       "3  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "4  T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...   \n",
       "1  T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...   \n",
       "2  Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...   \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...   \n",
       "4  ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...   \n",
       "\n",
       "                                             content  \n",
       "0  S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...  \n",
       "1  Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...  \n",
       "2  S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...  \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...  \n",
       "4  L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('https://github.com/honghanhh/mci_python_36a1_l2/raw/main/Lectures/lecture_09/data.json')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513f1e-0125-4cdc-a268-883af940025b",
   "metadata": {
    "id": "aa513f1e-0125-4cdc-a268-883af940025b"
   },
   "source": [
    "#### 3. M√£ h√≥a d·ªØ li·ªáu text (Word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8aaf76-389f-482d-a898-0987c1056da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b347065-59af-414d-8fee-f37389be5b5d",
   "metadata": {
    "id": "3b347065-59af-414d-8fee-f37389be5b5d"
   },
   "source": [
    "### Khai ph√° d·ªØ li·ªáu text (Text Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21cbd5e-23d3-44bf-93f0-f64def20cc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times', # best times\n",
    "    'It was the worst of times', # worst times\n",
    "    'It was the age of wisdom', # age wisdom\n",
    "    'It was the age of foolishness' # age foolishness\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
   "metadata": {
    "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f"
   },
   "source": [
    "#### Kh√°i ni·ªám Bag-of-Words\n",
    "\n",
    "L√† k·ªπ thu·∫≠t chia vƒÉn b·∫£n th√†nh c√°c t·ªï h·ª£p t·ª´ kh√°c nhau (b·∫±ng ph∆∞∆°ng ph√°p tokenize). C√°ch chia ph·ªï bi·∫øn l√† m·ªói c√¢u th√†nh 1 vƒÉn b·∫£n (bag) v√† m·ªói vƒÉn b·∫£n ƒë∆∞·ª£c chia th√†nh t·ª´ (word). D·ª±a v√†o ƒë√≥ c√≥ th·ªÉ ƒëo l∆∞·ªùng m·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong vƒÉn b·∫£n v√† x√¢y d·ª±ng m·ªëi li√™n h·ªá gi·ªØa ng·ªØ c·∫£nh v√† c√°c t·ª´. \n",
    "\n",
    "Hai y·∫øu t·ªë:\n",
    "1. T·ª´ ƒëi·ªÉn c·ªßa c√°c t·ª´ ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "2. M·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong t·ª´ ƒëi·ªÉn\n",
    "*M·ªói t·ª´ hay token ƒë∆∞·ª£c g·ªçi l√† m·ªôt `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vKTTqu9ccnHq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1637329798399,
     "user": {
      "displayName": "Samuel Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbcdQ40FcyCSIyitBN2ptY3zHWA_09harKPYF2=s64",
      "userId": "04384888964432338542"
     },
     "user_tz": -420
    },
    "id": "vKTTqu9ccnHq",
    "outputId": "b3daeb63-d877-4c17-b270-e435c4c76917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dict_vector = set()\n",
    "tokenized = [nltk.word_tokenize(i.lower()) for i in sentences]\n",
    "for i in tokenized:\n",
    "    for j in i:\n",
    "        dict_vector.add(j)\n",
    "        \n",
    "dictionary = [] # th√™m d·∫ßn v√†o t·ª´ ƒëi·ªÉn\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent) # tokenize\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "            \n",
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
   "metadata": {
    "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3"
   },
   "source": [
    "#### TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Ng·ªØ c·∫£nh: \"I am very angry\" ==> \"very angry\" # t·∫≠p trung v√†o c√°c t·ª´ mang nhi·ªÅu th√¥ng tin\n",
    "\n",
    "ƒê∆°n v·ªã ƒë·ªÉ ƒëo th√¥ng tin trong khoa h·ªçc machine-learning: entropy\n",
    "\n",
    "ƒêo l∆∞·ªùng t·∫ßn su·∫•t ***h·ª£p l√Ω*** m·ªôt t·ª´ (hay token) xu·∫•t hi·ªán trong vƒÉn b·∫£n. T·∫ßn su·∫•t n√†y ƒë∆∞·ª£c t√≠nh b·∫±ng: M·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n chia cho t·ªâ l·ªá vƒÉn b·∫£n m√† t·ª´ ƒë√≥ xu·∫•t hi·ªán tr√™n t·ªïng t·∫•t c·∫£ s·ªë l∆∞·ª£ng vƒÉn b·∫£n.\n",
    "\n",
    "V√≠ d·ª•: ƒë·ªëi v·ªõi nh∆∞ `what` hay `the`, c√°c t·ª´ n√†y xu·∫•t hi·ªán r·∫•t nhi·ªÅu tuy nhi√™n ko mang nhi·ªÅu √Ω nghƒ©a n√™n c·∫ßn c√≥ ph∆∞∆°ng ph√°p lo·∫°i tr·ª´ c√°c t·ª´ n√†y ra kh·ªèi m√¥ h√¨nh. V√¨ v·∫≠y ngo√†i t√≠nh to√°n m·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong vƒÉn b·∫£n, tuy nhi√™n n·∫øu vƒÉn b·∫£n n√†o c≈©ng xu·∫•t hi·ªán t·ª´ n√†y (ho·∫∑c ƒë∆°n gi·∫£n l√† r·∫•t nhi·ªÅu > 90%) th√¨ c√°c t·ª´ n√†y s·∫Ω b·ªã lo·∫°i ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: t·ª´ (word hay token)\n",
    " \n",
    "> `d`: vƒÉn b·∫£n (document)\n",
    " \n",
    "> `D`: t·ªáp c√°c vƒÉn b·∫£n (documents)\n",
    "\n",
    "trong ƒë√≥:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285",
   "metadata": {
    "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   },
   "source": [
    "#### M√¥ h√¨nh Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29",
   "metadata": {
    "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29"
   },
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03",
   "metadata": {
    "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03"
   },
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e7484-2125-4bfa-8033-9f11341f6c93",
   "metadata": {
    "id": "124e7484-2125-4bfa-8033-9f11341f6c93"
   },
   "source": [
    "Hai lo·∫°i m√¥ h√¨nh Word2Vec: **CBOW** (Continuous Bag-of-Word) v√† **Skip-Gram**\n",
    "\n",
    "Continuous Bag of Words (CBOW): *nh√¨n h√¨nh (ng·ªØ c·∫£nh) ƒëo√°n ch·ªØ*\n",
    "\n",
    "Ng∆∞·ª£c l·∫°i, Skip-Gram: *nh√¨n ch·ªØ ƒëo√°n h√¨nh (ng·ªØ c·∫£nh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea233-3d03-4566-8876-153325d71cd9",
   "metadata": {
    "id": "03aea233-3d03-4566-8876-153325d71cd9"
   },
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea",
   "metadata": {
    "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea"
   },
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d16c807e-c950-44b4-923a-3da9c08f74e8",
    "eec9ba76-913e-4803-a39b-d6b537ed57bb",
    "a0d782c1-6302-4a8e-b8de-e936565beb95",
    "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
    "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
    "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   ],
   "name": "nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
